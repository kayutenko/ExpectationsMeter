{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from time import time\n",
    "from gensim import corpora, models, similarities\n",
    "from itertools import chain\n",
    "from _datetime import datetime\n",
    "from lxml import etree\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from math import log\n",
    "from pprint import pprint as pp\n",
    "from string import punctuation\n",
    "from collections import defaultdict, deque\n",
    "import pandas as pd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Reads the given file and creates a generator object returning one line at a time split by tabulation\"\"\"\n",
    "    data = pd.read_excel(filename)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\expectations_meter\\english_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = [word.strip() for word in f.readlines()]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    return [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text) if word not in stopwords + list(punctuation)]\n",
    "\n",
    "def preprocess(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokens = [lemmatize(sent) for sent in sentences]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_text(words, text):\n",
    "    for word in words:\n",
    "        text = text.replace(word, '[<' + word + '>]')\n",
    "    return text\n",
    "\n",
    "def search_words(keywords, text):\n",
    "    tokens = preprocess(text)\n",
    "    keywords = set(list(chain(*preprocess(keywords))))\n",
    "    intersection = list(keywords.intersection(set(list(chain(*tokens)))))\n",
    "    if intersection:\n",
    "        return intersection, mark_text(intersection, text), tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['leak'],\n 'The new macbook is rumored to had a blah. The [<leak>] shows that it is a really funny way',\n [['The', 'new', 'macbook', 'is', 'rumored', 'to', 'had', 'blah'],\n  ['The', 'leak', 'show', 'is', 'really', 'funny', 'way']])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_words('rumor leak leakage', 'The new macbook is rumored to had a blah. The leak shows that it is a really funny way')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}