{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from time import time\n",
    "from gensim import corpora, models, similarities\n",
    "from itertools import chain\n",
    "from _datetime import datetime\n",
    "from lxml import etree\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from math import log\n",
    "from pprint import pprint as pp\n",
    "from string import punctuation\n",
    "from collections import defaultdict, deque\n",
    "import pandas as pd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для чтения исходных данных Excel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Reads the given file and creates a generator object returning one line at a time split by tabulation\"\"\"\n",
    "    data = pd.read_excel(filename)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Препроцессинг**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\expectations_meter\\\\english_stopwords.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-28bbbf25b04f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.\\expectations_meter\\english_stopwords.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.\\\\expectations_meter\\\\english_stopwords.txt'"
     ]
    }
   ],
   "source": [
    "with open('.\\expectations_meter\\english_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = [word.strip() for word in f.readlines()]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "numbers = re.compile('[0-9]')\n",
    "punctuation += '\\n—–- «»\\'\\\"'\n",
    "\n",
    "def lemmatize(text):\n",
    "    return [lemmatizer.lemmatize(word).lower() for word in nltk.word_tokenize(text) \n",
    "            if word not in stopwords + list(punctuation)\n",
    "            and word not in [\"''\", ' ', '``', '', \"'s\"]\n",
    "            ]\n",
    "\n",
    "def preprocess(text):\n",
    "    # sentences = nltk.sent_tokenize(text)\n",
    "    tokens = lemmatize(text.replace('\\n', ' '))\n",
    "    # [tokens.extend(lemmatize(sent)) for sent in sentences]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для получения токенизированных и лемматизированных данных и дат из корпуса**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corpus(filename):\n",
    "    # data = read_data(filename).iloc[:1000]\n",
    "    data = read_data(filename)\n",
    "    dates = data['datetime']\n",
    "    # bodies = [preprocess(text) for text in list(data['body']) if not pd.isnull(text)]\n",
    "    # titles = [preprocess(text) for text in list(data['title']) if not pd.isnull(text)]\n",
    "    bodies = []\n",
    "    for index, (title, body, author, datetime) in data.iterrows():\n",
    "        if not pd.isnull(body):\n",
    "            bodies.append(preprocess(body))\n",
    "    return bodies, dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Функции для поиска ключевых слов в тексте **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mark_text(words, text):\n",
    "    for word in words:\n",
    "        text = text.replace(word, '[<' + word + '>]')\n",
    "    return text\n",
    "\n",
    "def search_words(keywords, text):\n",
    "    tokens = preprocess(text)\n",
    "    keywords = set(list(chain(*preprocess(keywords))))\n",
    "    intersection = list(keywords.intersection(set(list(chain(*tokens)))))\n",
    "    if intersection:\n",
    "        return intersection, mark_text(intersection, text), tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_products(tokenized_text, window, product_list, year):\n",
    "    product_mentions =  set()\n",
    "    for i in range(window, len(tokenized_text)):\n",
    "        current_window = set(tokenized_text[i-window:i])\n",
    "        for product in product_list[year]:\n",
    "            intersection = current_window.intersection(set(product)) \n",
    "            if len(intersection) >= 1:\n",
    "                product_mentions.add(tuple(product))\n",
    "    return product_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Список всех продуктов**\n",
    "\n",
    "Считываем список всех продуктов Apple из стороннего файла. Создаем список из tuples. Убираем все упоминания числа гигабайт в моделях, а так же типов сотовой связи, и времени выхода в году"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_product_tuple(product_full_name):\n",
    "    name_cleaned = ''.join([char for char in product_full_name.lower() if not char in '()\\'\"&']).split()\n",
    "    parts_to_remove = ['gb', '+', 'late', 'early', 'mid', 'with', 'wcdma', 'cdma', 'gsm', '16', '32', '64', '128']\n",
    "    name_cleaned = tuple([re.sub('([0-9])(rd|st|nd|th)', '\\\\1', word) for word in name_cleaned if not word in parts_to_remove])\n",
    "    return name_cleaned\n",
    "\n",
    "def read_product_list(product_list_file):\n",
    "    models = defaultdict(list)\n",
    "    product_list = read_data(product_list_file)\n",
    "    for i, (year, launched, date_launched, model, family, discontinued) in product_list.iterrows():\n",
    "        models[year].append(make_product_tuple(model))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Поиск упоминаний продуктов из списка в корпусе**.\n",
    "\n",
    "Проходим по каждому тексту окном в 7 токенов. Если в них встречается пересечние с любым из продуктов аорзмером 2 и больше слов, то считаем, что продукт упоминается, и добавляем его в список упоминаний для данного продукта и данной даты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_mentions(dates, texts):\n",
    "    mentions = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "    for date, (id, text) in zip(dates, enumerate(texts)):\n",
    "        year = date[:4]\n",
    "        date = date[:10]\n",
    "        products_mentioned = search_products(text, 4, product_list, int(year))\n",
    "        for product in products_mentioned:\n",
    "            product_name = ' '.join(list(product))\n",
    "            # mentions[product_name][date]['number'] += 1\n",
    "            if 'text_ids' in  mentions[product_name][date]:\n",
    "                if id not in mentions[product_name][date]['text_ids']:\n",
    "                    mentions[product_name][date]['text_ids'].append(id)\n",
    "                    mentions[product_name][date]['number'] += 1\n",
    "            else:\n",
    "                mentions[product_name][date]['text_ids'] = [id]\n",
    "                mentions[product_name][date]['number'] += 1\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Визуализация**\n",
    "\n",
    "Визуализируем распределение упоминаний продуктов по датам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_mentions(mentions, dates):\n",
    "    width = 12\n",
    "    height = 8\n",
    "    plt.figure(figsize=(width, height))\n",
    "    new_dates = [i for i, d in enumerate(dates)]\n",
    "    legend = []\n",
    "    for product in mentions:\n",
    "        if 'macbook pro' in product:\n",
    "            ment_numbers = [mentions[product][date[:10]]['number'] if date[:10] in mentions[product] else 0 for date in dates]\n",
    "            plt.plot(new_dates, ment_numbers)\n",
    "            legend.append(product)\n",
    "    plt.legend(legend)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Собираем все вместе**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_list = read_product_list('.\\expectations_meter\\DATA\\Apple_produt_list_devices_only.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# app_ins_bodies, app_ins_dates = get_corpus('.\\expectations_meter\\DATA\\AppleInsider.xlsx')\n",
    "app_ins_dates = pd.read_excel('.\\expectations_meter\\DATA\\AppleInsider.xlsx')['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# appins_mentions = find_mentions(app_ins_dates[:2000], app_ins_bodies[:2000])\n",
    "# plot_mentions(appins_mentions, app_ins_dates[:2000])\n",
    "# appins_mentions"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
