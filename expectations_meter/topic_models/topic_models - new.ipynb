{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Парсинг дат**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "from time import time\n",
    "from gensim import corpora, models, similarities\n",
    "from itertools import chain\n",
    "from _datetime import datetime\n",
    "from lxml import etree\n",
    "import json\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from math import log\n",
    "from pprint import pprint as pp\n",
    "from string import punctuation\n",
    "from collections import defaultdict, deque, OrderedDict\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-11-02 2016-03-10 1224\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parse_date(date_string, product_list=False):\n",
    "    # date_only = date_string[:10]\n",
    "    # '2009-01-01 08:25:07'\n",
    "    # 2012-01-17T17:57:00-05:00\n",
    "    # Friday November 10, 2006 10:49 pm PST\n",
    "    # November 2, 2012 - product list\n",
    "    if not product_list:\n",
    "        if 'T' not in date_string:\n",
    "            date = datetime.strptime(date_string, '%Y-%m-%d %H:%M:%S').date()\n",
    "        elif 'pst' in date_string.lower() or 'pdt' in date_string.lower():\n",
    "            date = datetime.strptime(date_string[:-4], '%A %B %d, %Y %H:%M %p').date()\n",
    "        else:\n",
    "            date = datetime.strptime(date_string[:-6], '%Y-%m-%dT%H:%M:%S').date() \n",
    "    else:\n",
    "        date = datetime.strptime(date_string, '%B %d, %Y').date()\n",
    "    return date\n",
    "date = parse_date('November 2, 2012', True)\n",
    "article_date = parse_date('Friday March 10, 2016 10:49 pm PST')\n",
    "\n",
    "print(date, article_date, (article_date - date).days)\n",
    "def parse_dates(dates):\n",
    "    return [parse_date(date) for date in dates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Препроцессинг**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Reads the given file and creates a generator object returning one line at a time split by tabulation\"\"\"\n",
    "    data = pd.read_excel(filename)\n",
    "    return data\n",
    "\n",
    "with open('..\\english_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = [word.strip() for word in f.readlines()]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "numbers = re.compile('[0-9]')\n",
    "punctuation += '\\n—–- «»\\'\\\"'\n",
    "lemmatization_stoplist = ['3gs', '4s', '5s', '6s']\n",
    "product_lemm_reg = re.compile(\"(iphone|ipad|ipod|macbook|[ie]mac|air|5c|classic|nano|tv|ibook|shuffle|touch|mini|pro|powerbook)(s|es)\")\n",
    "\n",
    "def lemmatize(text, allow_stopwords=False):\n",
    "    if allow_stopwords:\n",
    "        current_stopwords = []\n",
    "    else:\n",
    "        current_stopwords = stopwords[:]\n",
    "    lemmatized = [lemmatizer.lemmatize(word).lower() \n",
    "                  if not word in lemmatization_stoplist else word.lower() \n",
    "                  for word in nltk.word_tokenize(text) \n",
    "                    if word not in current_stopwords + list(punctuation)\n",
    "                    and word not in [\"''\", ' ', '``', '', \"'s\"]\n",
    "                 ]\n",
    "    product_lemmatized = [product_lemm_reg.sub('\\\\1', word) for word in lemmatized]\n",
    "    return product_lemmatized\n",
    "\n",
    "def preprocess_text(text, split=None, allow_stopwords=False):\n",
    "    if split == 'sentence':\n",
    "        sentecnes = nltk.sent_tokenize(text)\n",
    "        return [lemmatize(sent, allow_stopwords) for sent in elements]\n",
    "    elif split == 'paragraph_sentence':\n",
    "        try:\n",
    "            paragraphs = [nltk.sent_tokenize(par) for par in re.split('[\\r\\n]{2,}', text) if not par.strip() == '']\n",
    "        except:\n",
    "            print(text)\n",
    "        return [[lemmatize(sent, allow_stopwords) for sent in sents if not sent.strip() == ''] for sents in paragraphs]\n",
    "    elif split == 'paragraph':\n",
    "        paragraphs = [lemmatize(par, allow_stopwords) for par in re.split('[\\r\\n]{2,}', text)]\n",
    "        return paragraphs\n",
    "    else:\n",
    "        return lemmatize(text)\n",
    "    \n",
    "def make_corpus(filename, serialize=None):\n",
    "    data = read_data(filename)\n",
    "    dates = parse_dates(data['datetime'])\n",
    "    preprocessed_docs =  [[id, date, preprocess_text(body, split='paragraph_sentence')] for (id, body), date in zip(enumerate(list(data['body'])), dates) if not pd.isnull(body) and not body.strip() == '']\n",
    "    if serialize:\n",
    "        pickle_serialize(preprocessed_docs, serialize)\n",
    "    return preprocessed_docs\n",
    "\n",
    "def pickle_serialize(obj, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(pickle.dumps(obj))\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.loads(f.read())\n",
    "    \n",
    "# test_corpus = make_corpus('../DATA/test_data.xlsx')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим и предобработаем все корпуса. \n",
    "Затем склеим их в один большой корпус, чтобы построить по нему тематическую модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# app_ins_corpus = make_corpus('../DATA/AppleInsider.xlsx', serialize = '../DATA/AppleInsiderPreprocessed.pickle')\n",
    "# app_ins_corpus = [elt for elt in app_ins_corpus if elt[1].year >= 2009]\n",
    "# pickle_serialize(app_ins_corpus, '../DATA/AppleInsiderPreprocessed.pickle')\n",
    "app_ins_corpus = load_pickle('../DATA/AppleInsiderPreprocessed.pickle')\n",
    "# app_ins_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " datetime.date(2009, 1, 1),\n",
       " [[['addressing',\n",
       "    'complaint',\n",
       "    'nearly',\n",
       "    'every',\n",
       "    'iphone',\n",
       "    'owner',\n",
       "    'in',\n",
       "    'northern',\n",
       "    'climate',\n",
       "    'apple',\n",
       "    'ha',\n",
       "    'filed',\n",
       "    'patent',\n",
       "    'would',\n",
       "    'cover',\n",
       "    'mean',\n",
       "    'using',\n",
       "    'multi-touch',\n",
       "    'device',\n",
       "    'with',\n",
       "    'glove'],\n",
       "   ['the',\n",
       "    'us',\n",
       "    'patent',\n",
       "    'office',\n",
       "    'application',\n",
       "    'note',\n",
       "    'capacitive',\n",
       "    'touchcreen',\n",
       "    'like',\n",
       "    'on',\n",
       "    'iphone',\n",
       "    'ipod',\n",
       "    'touch',\n",
       "    'are',\n",
       "    'problematic',\n",
       "    'in',\n",
       "    'colder',\n",
       "    'weather'],\n",
       "   ['as',\n",
       "    'depend',\n",
       "    'on',\n",
       "    'electrical',\n",
       "    'response',\n",
       "    'from',\n",
       "    'user',\n",
       "    'fingertip',\n",
       "    'is',\n",
       "    'often',\n",
       "    'blocked',\n",
       "    'when',\n",
       "    'wearing',\n",
       "    'insulated',\n",
       "    'glove',\n",
       "    'screen',\n",
       "    'either',\n",
       "    'force',\n",
       "    'user',\n",
       "    'to',\n",
       "    'take',\n",
       "    'glove',\n",
       "    'else',\n",
       "    'sit',\n",
       "    'tight',\n",
       "    'return',\n",
       "    'indoors']],\n",
       "  [['apple',\n",
       "    'solution',\n",
       "    'would',\n",
       "    'give',\n",
       "    'glove',\n",
       "    'second',\n",
       "    'inner',\n",
       "    'layer',\n",
       "    'beyond',\n",
       "    'surface',\n",
       "    'would',\n",
       "    'simulate',\n",
       "    'electrical',\n",
       "    'feedback',\n",
       "    'human',\n",
       "    'finger',\n",
       "    'when',\n",
       "    'exposed',\n",
       "    'to',\n",
       "    'outside'],\n",
       "   ['apertures',\n",
       "    'fingertip',\n",
       "    'would',\n",
       "    'let',\n",
       "    'user',\n",
       "    'peel',\n",
       "    'back',\n",
       "    'outer',\n",
       "    'weatherproof',\n",
       "    'layer',\n",
       "    'to',\n",
       "    'leave',\n",
       "    'finger',\n",
       "    'protected',\n",
       "    'inner',\n",
       "    'layer',\n",
       "    'capable',\n",
       "    'using',\n",
       "    'touchcreen',\n",
       "    'device',\n",
       "    'with',\n",
       "    'roughly',\n",
       "    'responsiveness',\n",
       "    'bare',\n",
       "    'skin']],\n",
       "  [['the',\n",
       "    'technique',\n",
       "    'could',\n",
       "    'use',\n",
       "    'either',\n",
       "    'elastic',\n",
       "    'ring',\n",
       "    'to',\n",
       "    'open',\n",
       "    'close',\n",
       "    'opening',\n",
       "    'on',\n",
       "    'fly',\n",
       "    'could',\n",
       "    'have',\n",
       "    'protective',\n",
       "    'cap',\n",
       "    'instead']],\n",
       "  [['unlike',\n",
       "    'many',\n",
       "    'apple',\n",
       "    'patent',\n",
       "    'filing',\n",
       "    'basic',\n",
       "    'goal',\n",
       "    'glove',\n",
       "    'is',\n",
       "    \"n't\",\n",
       "    'unique',\n",
       "    'clothier',\n",
       "    'dots',\n",
       "    'have',\n",
       "    'developed',\n",
       "    'glove',\n",
       "    'also',\n",
       "    'try',\n",
       "    'to',\n",
       "    'generate',\n",
       "    'conductivity'],\n",
       "   ['in',\n",
       "    'case',\n",
       "    'though',\n",
       "    'often',\n",
       "    'either',\n",
       "    'limit',\n",
       "    'size',\n",
       "    'contact',\n",
       "    'point',\n",
       "    'else',\n",
       "    'make',\n",
       "    'conductive',\n",
       "    'surface',\n",
       "    'part',\n",
       "    'single',\n",
       "    'layer',\n",
       "    'thus',\n",
       "    'reduce',\n",
       "    'protection',\n",
       "    'glove',\n",
       "    'against',\n",
       "    'cold']],\n",
       "  [['credited',\n",
       "    'to',\n",
       "    'inventor',\n",
       "    'steven',\n",
       "    'hotelling',\n",
       "    'ashwin',\n",
       "    'sunder',\n",
       "    'known',\n",
       "    'whether',\n",
       "    'submitted',\n",
       "    'patent',\n",
       "    'reveals',\n",
       "    'apple',\n",
       "    'plan'],\n",
       "   ['the',\n",
       "    'company',\n",
       "    'rarely',\n",
       "    'venture',\n",
       "    'fabric',\n",
       "    'accessory',\n",
       "    'ipod',\n",
       "    'sock',\n",
       "    'pouch',\n",
       "    'have',\n",
       "    'come',\n",
       "    'with',\n",
       "    'ipod'],\n",
       "   ['the',\n",
       "    'application',\n",
       "    'wa',\n",
       "    'originally',\n",
       "    'filed',\n",
       "    'on',\n",
       "    'june',\n",
       "    '28th',\n",
       "    '2007',\n",
       "    'day',\n",
       "    'before',\n",
       "    'original',\n",
       "    'iphone',\n",
       "    'launch']]]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_ins_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mcr_corpus = make_corpus('../DATA/MacRummors.xlsx', serialize = '../DATA/MacRummorsPreprocessed.pickle')\n",
    "# mcr_corpus = [elt for elt in mcr_corpus if elt[1].year >= 2009]\n",
    "# pickle_serialize(mcr_corpus, '../DATA/MacRummorsPreprocessed.pickle')\n",
    "mcr_corpus = load_pickle('../DATA/MacRummorsPreprocessed.pickle')\n",
    "# mcr_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nfm_corpus = make_corpus('../DATA/NineToFiveMac.xlsx', serialize = '../DATA/NineToFiveMacPreprocessed.pickle')\n",
    "nfm_corpus = load_pickle('../DATA/NineToFiveMacPreprocessed.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_corpus = defaultdict(list)\n",
    "[combined_corpus[date].append(pars) for id, date, pars in app_ins_corpus]\n",
    "[combined_corpus[date].append(pars) for id, date, pars in mcr_corpus]\n",
    "[combined_corpus[date].append(pars) for id, date, pars in nfm_corpus]\n",
    "cc = []\n",
    "for date, docs in sorted(combined_corpus.items(), key=lambda x: x[0]):\n",
    "    for pars in docs:\n",
    "        for par in pars:\n",
    "            cc.append(list(chain(*par)))\n",
    "\n",
    "# combined_corpus = [[id, date, pars] for id, (date, pars) in enumerate(sorted(combined_corpus.items(), key=lambda x: x[0]))]\n",
    "# combined_doc_corpus = defaultdict(list)\n",
    "# [combined_doc_corpus[date].append(doc) for id, date, doc in app_ins_corpus]\n",
    "# [combined_doc_corpus[date].append(doc) for id, date, doc in mcr_corpus]\n",
    "# [combined_doc_corpus[date].append(doc) for id, date, doc in nfm_corpus]\n",
    "# cc_docs = []\n",
    "# id = 0\n",
    "# for date, docs in sorted(combined_doc_corpus.items(), key=lambda x: x[0]):\n",
    "#     for doc in docs:\n",
    "#         cc_docs.append([id, date, doc])\n",
    "#         id += 1\n",
    "# cc_docs[:10]\n",
    "# [[id, date, doc for doc in docs] for id, (date, docs) in enumerate(sorted(combined_doc_corpus.items(), key=lambda x: x[0]))]\n",
    "# del(app_ins_corpus, mcr_corpus, nfm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as',\n",
       " 'previously',\n",
       " 'known',\n",
       " 'iphone',\n",
       " 'will',\n",
       " 'be',\n",
       " 'available',\n",
       " 'in',\n",
       " 'june',\n",
       " 'exact',\n",
       " 'date',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'announced',\n",
       " 'the',\n",
       " 'document',\n",
       " 'also',\n",
       " 'reinforces',\n",
       " '499',\n",
       " '599',\n",
       " 'price',\n",
       " 'iphone',\n",
       " 'instructs',\n",
       " 'employee',\n",
       " 'to',\n",
       " 'speculate',\n",
       " 'discus',\n",
       " 'pricing',\n",
       " 'possibilties.related',\n",
       " 'roundup',\n",
       " 'iphone',\n",
       " '6s',\n",
       " 'buyer',\n",
       " 'guide',\n",
       " 'iphone',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "cc[23002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подготавливаем данные для построения тематической модели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def make_batches(corpus ,test=False):\n",
    "    if test:\n",
    "        first_batch_len = 100\n",
    "    else:\n",
    "        first_batch_len = 2000\n",
    "    batch_id = 0\n",
    "    batches = []\n",
    "    prev_date = corpus[0][1]\n",
    "    current_batch = []\n",
    "    for id, date, paragraphs in corpus:\n",
    "        if batch_id != 0:\n",
    "            current_batch.extend([list(chain(*sents)) for sents in paragraphs])\n",
    "            if len(current_batch) >= 30:\n",
    "                batches.append([date, current_batch])\n",
    "                current_batch = []\n",
    "        else:\n",
    "            if len(current_batch) < first_batch_len:\n",
    "                current_batch.extend([list(chain(*sents)) for sents in paragraphs])\n",
    "            else:\n",
    "                prev_date = date\n",
    "                batch_id += 1\n",
    "    return batches\n",
    "\n",
    "def make_dictionary(texts, test=False):\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    if test:\n",
    "        dictionary.filter_extremes(no_below=1, no_above=1000, keep_n=None)\n",
    "    else:\n",
    "        dictionary.filter_extremes(no_below=20, no_above=0.5, keep_n=None)\n",
    "    print(dictionary)\n",
    "    return  dictionary\n",
    "\n",
    "def make_bow_texts(batch, dictionary):\n",
    "    bow_texts = [dictionary.doc2bow(text) for text in batch]\n",
    "    return bow_texts\n",
    "\n",
    "# test_batches = make_batches(test_corpus, test=True)\n",
    "# test_texts = [list(chain(*chain(*doc[2]))) for doc in test_corpus]\n",
    "# test_dict = make_dictionary(test_texts, test=True) \n",
    "# test_bows = [[date, make_bow_texts(batch, test_dict)] for (date, batch) in test_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : built Dictionary(181592 unique tokens: ['batter-by-batter', 'homegrown', 'june.we', 'codifying', 'users.verizon']...) from 5462 documents (total 13201508 corpus positions)\n",
      "INFO : discarding 165640 tokens: [('crabb', 5), ('i', 2856), ('note', 3198), ('we', 3035), ('on', 5069), ('even', 3169), ('well', 3598), ('//www.macinstein.com/pressrelease.cfm', 1), ('rachel', 10), ('saraceno', 1)]...\n",
      "INFO : keeping 15952 tokens which were in no less than 20 and no more than 2731 (=50.0%) documents\n",
      "INFO : resulting dictionary: Dictionary(15952 unique tokens: ['shifted', 'varies', 'reaching', 'glossy', 'modifying']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(15952 unique tokens: ['shifted', 'varies', 'reaching', 'glossy', 'modifying']...)\n"
     ]
    }
   ],
   "source": [
    "cc_batches = make_batches(combined_corpus) # объединенный корпус, разбитый на батчи с документами за каждый день\n",
    "cc_texts = [list(chain(*chain(*doc[2]))) for doc in combined_corpus] # просто список всех текстов во всем корпусе\n",
    "cc_dict = make_dictionary(cc_texts)  #  словарь id2word слов на всем корпусе с\n",
    "cc_bows =  [[date, make_bow_texts(batch, cc_dict)] for date, batch in cc_batches]  # весь корпус с параграфами переведенными в формат bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Строим модель**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(model, corpus):\n",
    "    \"\"\"Calculates perpelexity of the given model on a given corpus of bow texts\"\"\"\n",
    "    corpus_length = 0\n",
    "    log_likelihood = 0\n",
    "    topic_profiles = model.state.get_lambda() / np.sum(model.state.get_lambda(), axis=1)[:, np.newaxis]\n",
    "    for document in corpus:\n",
    "        gamma, _ = model.inference([document])\n",
    "        document_profile = gamma / np.sum(gamma)\n",
    "        for term_id, term_count in document:\n",
    "            corpus_length += term_count\n",
    "            term_probability = np.dot(document_profile, topic_profiles[:, term_id])\n",
    "            log_likelihood += term_count * log(term_probability)\n",
    "    perplexity = np.exp(-log_likelihood / corpus_length)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_batch_model(dictionary, bow_batches, num_topics=400, chunksize=500, passes=10, batch=True, name= 'topics_' + str(datetime.now()).replace(':', '-')[:19]):   \n",
    "    start = datetime.now()\n",
    "    model = models.ldamulticore.LdaMulticore(id2word=dictionary, \n",
    "                                                     num_topics=num_topics, \n",
    "                                                     workers=3,\n",
    "                                                     chunksize=chunksize,  \n",
    "                                                     passes=passes,\n",
    "                                                     iterations = 200\n",
    "                                                     )\n",
    "    os.makedirs(os.path.join('.', name), exist_ok = True)\n",
    "    n_batches = len(bow_batches)\n",
    "    for i, (date, batch) in enumerate(bow_batches):\n",
    "        if i and i % 100 == 0: \n",
    "            print(\"{} of {} batches processed\".format(i, n_batches))\n",
    "        model.update(batch)\n",
    "        if i % 7 == 0:\n",
    "            with open(os.path.join('.', name, str(date)), 'wb') as f:\n",
    "                f.write(pickle.dumps(model))\n",
    "    with open(os.path.join('.', name, str(date)), 'wb') as f:\n",
    "                f.write(pickle.dumps(model))\n",
    "    print('Evaluation time: {}'.format((datetime.now() - start) / 60))\n",
    "#     print('Perplexity: {}'.format(perplexity(model, bow_batches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model(dictionary, doc_bows, num_topics=400, chunksize=500, passes=10, batch=True, name= 'topics_' + str(datetime.now()).replace(':', '-')[:19]):   \n",
    "    start = datetime.now()\n",
    "    model = models.ldamulticore.LdaMulticore(doc_bows,\n",
    "                                             id2word=dictionary, \n",
    "                                             num_topics=num_topics, \n",
    "                                             workers=3,\n",
    "                                             chunksize=chunksize,  \n",
    "                                             passes=passes,\n",
    "                                             iterations = 100\n",
    "                                             )\n",
    "    os.makedirs(os.path.join('.', name), exist_ok = True)\n",
    "    model.save(os.path.join('.', name, 'model'))\n",
    "    print('Evaluation time: {}'.format((datetime.now() - start) / 60))\n",
    "    return model\n",
    "#     print('Perplexity: {}'.format(perplexity(model, bow_batches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : adding document #10000 to Dictionary(14025 unique tokens: ['shifted', 'slotted', 'reaching', 'afterdark.com', 'previously-rumored']...)\n",
      "INFO : adding document #20000 to Dictionary(22293 unique tokens: ['shifted', 'slotted', 'reaching', 'afterdark.com', 'homegrown']...)\n",
      "INFO : adding document #30000 to Dictionary(30734 unique tokens: ['shifted', 'slotted', 'reaching', 'afterdark.com', 'modifying']...)\n",
      "INFO : adding document #40000 to Dictionary(39095 unique tokens: ['shifted', 'slotted', 'reaching', 'dynamically.our', 'afterdark.com']...)\n",
      "INFO : adding document #50000 to Dictionary(47977 unique tokens: ['40.3', 'slotted', 'reaching', 'afterdark.com', '149.28']...)\n",
      "INFO : adding document #60000 to Dictionary(55019 unique tokens: ['40.3', 'slotted', 'applicationfirewall', 'reaching', 'afterdark.com']...)\n",
      "INFO : adding document #70000 to Dictionary(60428 unique tokens: ['40.3', 'slotted', 'applicationfirewall', 'reaching', 'afterdark.com']...)\n",
      "INFO : adding document #80000 to Dictionary(65392 unique tokens: ['40.3', 'slotted', 'applicationfirewall', 'reaching', 'afterdark.com']...)\n",
      "INFO : adding document #90000 to Dictionary(71292 unique tokens: ['40.3', 'slotted', 'applicationfirewall', 'reaching', 'afterdark.com']...)\n",
      "INFO : adding document #100000 to Dictionary(76010 unique tokens: ['40.3', 'slotted', 'applicationfirewall', 'reaching', 'afterdark.com']...)\n",
      "INFO : adding document #110000 to Dictionary(80984 unique tokens: ['40.3', 'slotted', 'applicationfirewall', 'reaching', 'afterdark.com']...)\n",
      "INFO : adding document #120000 to Dictionary(85956 unique tokens: ['40.3', 'slotted', 'applicationfirewall', 'reaching', 'afterdark.com']...)\n",
      "INFO : adding document #130000 to Dictionary(90647 unique tokens: ['slotted', '149.28', 'homegrown', 'ocz', 'users.verizon']...)\n",
      "INFO : adding document #140000 to Dictionary(95631 unique tokens: ['slotted', '149.28', 'homegrown', 'ocz', 'users.verizon']...)\n",
      "INFO : adding document #150000 to Dictionary(100412 unique tokens: ['slotted', '149.28', 'homegrown', 'ocz', 'users.verizon']...)\n",
      "INFO : adding document #160000 to Dictionary(105022 unique tokens: [\"'414\", 'slotted', '149.28', 'homegrown', 'ocz']...)\n",
      "INFO : adding document #170000 to Dictionary(109818 unique tokens: [\"'414\", 'slotted', '149.28', 'homegrown', 'ocz']...)\n",
      "INFO : adding document #180000 to Dictionary(115013 unique tokens: [\"'414\", 'slotted', '149.28', 'homegrown', 'ocz']...)\n",
      "INFO : adding document #190000 to Dictionary(119342 unique tokens: [\"'414\", 'slotted', \"'yep\", '149.28', 'homegrown']...)\n",
      "INFO : adding document #200000 to Dictionary(123129 unique tokens: [\"'414\", 'slotted', \"'yep\", '149.28', 'homegrown']...)\n",
      "INFO : adding document #210000 to Dictionary(126926 unique tokens: [\"'414\", 'slotted', \"'yep\", '149.28', 'homegrown']...)\n",
      "INFO : adding document #220000 to Dictionary(131169 unique tokens: [\"'414\", 'slotted', 'court-assigned', \"'yep\", '149.28']...)\n",
      "INFO : adding document #230000 to Dictionary(134725 unique tokens: [\"'414\", 'slotted', 'court-assigned', \"'yep\", '149.28']...)\n",
      "INFO : adding document #240000 to Dictionary(138482 unique tokens: [\"'414\", 'slotted', 'court-assigned', \"'yep\", '149.28']...)\n",
      "INFO : adding document #250000 to Dictionary(142354 unique tokens: [\"'414\", 'slotted', 'court-assigned', \"'yep\", '149.28']...)\n",
      "INFO : adding document #260000 to Dictionary(146420 unique tokens: [\"'414\", 'slotted', 'court-assigned', \"'yep\", '149.28']...)\n",
      "INFO : adding document #270000 to Dictionary(151342 unique tokens: [\"'414\", 'slotted', 'court-assigned', \"'yep\", '149.28']...)\n",
      "INFO : adding document #280000 to Dictionary(155465 unique tokens: [\"'414\", 'slotted', 'court-assigned', \"'yep\", '149.28']...)\n",
      "INFO : adding document #290000 to Dictionary(159227 unique tokens: [\"'414\", 'pelicans/warriors', 'slotted', 'court-assigned', \"'yep\"]...)\n",
      "INFO : adding document #300000 to Dictionary(162896 unique tokens: [\"'414\", 'pelicans/warriors', 'slotted', 'court-assigned', '2,827.95']...)\n",
      "INFO : adding document #310000 to Dictionary(166239 unique tokens: [\"'414\", 'pelicans/warriors', 'slotted', 'court-assigned', '2,827.95']...)\n",
      "INFO : adding document #320000 to Dictionary(169203 unique tokens: [\"'414\", 'pelicans/warriors', 'slotted', 'court-assigned', '2,827.95']...)\n",
      "INFO : adding document #330000 to Dictionary(172089 unique tokens: [\"'414\", 'pelicans/warriors', 'slotted', 'court-assigned', '2,827.95']...)\n",
      "INFO : adding document #340000 to Dictionary(174528 unique tokens: [\"'414\", 'pelicans/warriors', 'slotted', 'court-assigned', '2,827.95']...)\n",
      "INFO : adding document #350000 to Dictionary(176789 unique tokens: ['batter-by-batter', 'homegrown', 'june.we', 'codifying', 'users.verizon']...)\n",
      "INFO : adding document #360000 to Dictionary(179519 unique tokens: ['batter-by-batter', 'homegrown', 'june.we', 'codifying', 'users.verizon']...)\n",
      "INFO : built Dictionary(181592 unique tokens: ['batter-by-batter', 'homegrown', 'june.we', 'codifying', 'users.verizon']...) from 368712 documents (total 13201508 corpus positions)\n",
      "INFO : discarding 163229 tokens: [('crabb', 7), ('macweek', 17), ('articles/editorials.', 1), ('saraceno', 1), ('relics', 1), ('macinstein', 1), ('//www.macinstein.com/pressrelease.cfm', 1), ('id=268', 1), ('edi', 1), ('torial', 1)]...\n",
      "INFO : keeping 18363 tokens which were in no less than 20 and no more than 184356 (=50.0%) documents\n",
      "INFO : resulting dictionary: Dictionary(18363 unique tokens: ['shifted', 'varies', 'reaching', 'glossy', 'modifying']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(18363 unique tokens: ['shifted', 'varies', 'reaching', 'glossy', 'modifying']...)\n"
     ]
    }
   ],
   "source": [
    "cc_dict = make_dictionary(cc)\n",
    "cc_bows = make_bow_texts(cc, cc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1400, 1),\n",
       " (1832, 1),\n",
       " (1962, 1),\n",
       " (2067, 1),\n",
       " (3741, 1),\n",
       " (4842, 1),\n",
       " (5714, 1),\n",
       " (5988, 1),\n",
       " (6276, 2),\n",
       " (7816, 1),\n",
       " (8753, 1),\n",
       " (9105, 1),\n",
       " (9413, 1),\n",
       " (9514, 1),\n",
       " (10027, 2),\n",
       " (10205, 1),\n",
       " (10798, 1),\n",
       " (10882, 1),\n",
       " (10936, 1),\n",
       " (11153, 1),\n",
       " (12221, 1),\n",
       " (12687, 1),\n",
       " (13058, 1),\n",
       " (13551, 1),\n",
       " (13718, 1),\n",
       " (14115, 1),\n",
       " (14135, 1),\n",
       " (14597, 1),\n",
       " (14599, 1),\n",
       " (15079, 1),\n",
       " (15271, 1),\n",
       " (18084, 1)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_bows[1004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using symmetric alpha at 0.001\n",
      "INFO : using symmetric eta at 5.445733267984534e-05\n",
      "INFO : using serial LDA version on this node\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "model = make_model(cc_dict, cc_bows, num_topics = 1000, chunksize = 100, passes = 10, name='all_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model = make_model(test_dict, test_bows, num_topics = 10, chunksize = 5, passes = 10, name = 'test_model')\n",
    "# model\n",
    "# pickle.loads(list(model.items())[1][1]).show_topics()\n",
    "# alltime_model_dict = make_model(cc_dict, cc_bows, num_topics = 500, chunksize = 5, passes = 2, name='paragraphs_unconverged_model3')\n",
    "# with open('../DATA/by_day_topic_model.pickle', 'wb') as f:\n",
    "#     f.write(pickle.dumps(alltime_model_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{datetime.date(2009, 1, 6): 'test_model\\\\2009-01-06',\n",
       " datetime.date(2009, 1, 16): 'test_model\\\\2009-01-16',\n",
       " datetime.date(2009, 1, 19): 'test_model\\\\2009-01-19'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_topic_models(directory):\n",
    "    from datetime import datetime\n",
    "    models_dict = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        date = datetime.strptime(filename, '%Y-%m-%d').date()\n",
    "        models_dict[date] = os.path.join(directory, filename)\n",
    "    return models_dict\n",
    "\n",
    "models_dict = load_topic_models('test_model')\n",
    "models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start gathering\n",
      "model loaded\n",
      "[('also+offering+easy+dvd+graphic', 1)]\n",
      "[('ability+without+see+related+mean', 1)]\n",
      "[('order+request+single+find+saved', 1)]\n",
      "[('new+label+drm-free+store+million', 1)]\n",
      "[('order+request+single+find+saved', 1)]\n",
      "[('price+high+charge+three+window.adsbygoogle', 1)]\n",
      "[('order+request+single+find+saved', 2), ('photo+version+movie+definition+different', 1)]\n",
      "[('order+request+single+find+saved', 1), (\"pound+latest+removing+kg+'09\", 1)]\n",
      "[('order+request+single+find+saved', 3)]\n",
      "[('battery+to+macbook+model+new', 2), ('order+request+single+find+saved', 1), ('ability+without+see+related+mean', 1)]\n",
      "[('price+high+charge+three+window.adsbygoogle', 1)]\n",
      "2009-01-16\n",
      "[('series+that+needed+billion+fifth', 6), ('release+before+world+change+appleinsider', 2), ('exploit+malware+sandboxing+software+attack', 2)]\n",
      "[('series+that+needed+billion+fifth', 4)]\n",
      "[('series+that+needed+billion+fifth', 4), ('pc+move+system+writing+rather', 2), ('security+microsoft+latest+address+windows', 1)]\n",
      "[('series+that+needed+billion+fifth', 4), ('pc+move+system+writing+rather', 1)]\n",
      "[('series+that+needed+billion+fifth', 5), ('security+microsoft+latest+address+windows', 1), ('exploit+malware+sandboxing+software+attack', 1), ('access+security+installed+feature+new', 1)]\n",
      "[('series+that+needed+billion+fifth', 1)]\n",
      "[('series+that+needed+billion+fifth', 1), ('pc+move+system+writing+rather', 1)]\n",
      "[('series+that+needed+billion+fifth', 1), ('pc+move+system+writing+rather', 1)]\n",
      "[('pc+move+system+writing+rather', 3), ('release+before+world+change+appleinsider', 1)]\n",
      "[('series+that+needed+billion+fifth', 3)]\n",
      "[('series+that+needed+billion+fifth', 1)]\n",
      "[('series+that+needed+billion+fifth', 2), ('pc+move+system+writing+rather', 1)]\n",
      "[('series+that+needed+billion+fifth', 1), ('pc+move+system+writing+rather', 1)]\n",
      "[('series+that+needed+billion+fifth', 7), ('pc+move+system+writing+rather', 2)]\n",
      "[('series+that+needed+billion+fifth', 5), ('release+before+world+change+appleinsider', 1), ('target+effort+about+issue+focus', 1), ('pc+move+system+writing+rather', 1)]\n",
      "[('security+microsoft+latest+address+windows', 2), ('series+that+needed+billion+fifth', 2), ('release+before+world+change+appleinsider', 1)]\n",
      "[('series+that+needed+billion+fifth', 9), ('security+memory+attacker+malicious+user', 1), ('target+effort+about+issue+focus', 1)]\n",
      "[('series+that+needed+billion+fifth', 2), ('pc+move+system+writing+rather', 1)]\n",
      "[('series+that+needed+billion+fifth', 4)]\n",
      "[('series+that+needed+billion+fifth', 16), ('pc+move+system+writing+rather', 2), ('security+microsoft+latest+address+windows', 1), ('target+effort+about+issue+focus', 1)]\n",
      "2009-01-19\n",
      "[('new+when+anything+access+having', 3), (\"n't+however+against+up+give\", 1)]\n",
      "[('internet+microsoft+monopoly+software+enough', 3), ('new+when+anything+access+having', 1)]\n",
      "[('new+when+anything+access+having', 1), ('page+microsoft+windows+email+chief', 1)]\n",
      "[('new+when+anything+access+having', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.get_topic_distributions.<locals>.<lambda>>,\n",
       "            {'ability+without+see+related+mean': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 1): 1,\n",
       "                          datetime.date(2009, 1, 6): 1}),\n",
       "             'access+security+installed+feature+new': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 9): 1}),\n",
       "             'also+offering+easy+dvd+graphic': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 1): 1}),\n",
       "             'battery+to+macbook+model+new': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 6): 1}),\n",
       "             'exploit+malware+sandboxing+software+attack': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 7): 1,\n",
       "                          datetime.date(2009, 1, 9): 1}),\n",
       "             'internet+microsoft+monopoly+software+enough': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 19): 1}),\n",
       "             \"n't+however+against+up+give\": defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 17): 1}),\n",
       "             'new+label+drm-free+store+million': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 2): 1}),\n",
       "             'new+when+anything+access+having': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 17): 1,\n",
       "                          datetime.date(2009, 1, 19): 3}),\n",
       "             'order+request+single+find+saved': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 2): 2,\n",
       "                          datetime.date(2009, 1, 5): 3,\n",
       "                          datetime.date(2009, 1, 6): 1}),\n",
       "             'page+microsoft+windows+email+chief': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 19): 1}),\n",
       "             'pc+move+system+writing+rather': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 8): 1,\n",
       "                          datetime.date(2009, 1, 9): 1,\n",
       "                          datetime.date(2009, 1, 10): 1,\n",
       "                          datetime.date(2009, 1, 11): 1,\n",
       "                          datetime.date(2009, 1, 12): 1,\n",
       "                          datetime.date(2009, 1, 13): 3,\n",
       "                          datetime.date(2009, 1, 14): 1,\n",
       "                          datetime.date(2009, 1, 15): 1,\n",
       "                          datetime.date(2009, 1, 16): 1}),\n",
       "             'photo+version+movie+definition+different': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 5): 1}),\n",
       "             \"pound+latest+removing+kg+'09\": defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 5): 1}),\n",
       "             'price+high+charge+three+window.adsbygoogle': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 3): 1,\n",
       "                          datetime.date(2009, 1, 6): 1}),\n",
       "             'release+before+world+change+appleinsider': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 7): 1,\n",
       "                          datetime.date(2009, 1, 12): 1,\n",
       "                          datetime.date(2009, 1, 14): 1,\n",
       "                          datetime.date(2009, 1, 15): 1}),\n",
       "             'security+memory+attacker+malicious+user': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 15): 1}),\n",
       "             'security+microsoft+latest+address+windows': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 8): 1,\n",
       "                          datetime.date(2009, 1, 9): 1,\n",
       "                          datetime.date(2009, 1, 15): 1,\n",
       "                          datetime.date(2009, 1, 16): 1}),\n",
       "             'series+that+needed+billion+fifth': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 7): 1,\n",
       "                          datetime.date(2009, 1, 8): 2,\n",
       "                          datetime.date(2009, 1, 9): 3,\n",
       "                          datetime.date(2009, 1, 10): 1,\n",
       "                          datetime.date(2009, 1, 11): 1,\n",
       "                          datetime.date(2009, 1, 12): 2,\n",
       "                          datetime.date(2009, 1, 13): 3,\n",
       "                          datetime.date(2009, 1, 14): 1,\n",
       "                          datetime.date(2009, 1, 15): 3,\n",
       "                          datetime.date(2009, 1, 16): 2}),\n",
       "             'target+effort+about+issue+focus': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 14): 1,\n",
       "                          datetime.date(2009, 1, 15): 1,\n",
       "                          datetime.date(2009, 1, 16): 1})})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.loads(f.read())\n",
    "\n",
    "\n",
    "def get_topic_distributions(dictionary, data, model_dict):\n",
    "    import datetime\n",
    "    print('start gathering')\n",
    "    models_dates = list(model_dict.keys())\n",
    "#     print(sorted(models_dates))\n",
    "    topic_by_date_counts = defaultdict(lambda: defaultdict(int))\n",
    "    date_by_topic_counts = defaultdict(lambda: defaultdict(int))\n",
    "    current_model_date = min(models_dates)\n",
    "    model = load_model(model_dict[current_model_date])\n",
    "    print('model loaded')\n",
    "    for id, date, paragraphs in data:\n",
    "#         print('-------------------------\\ncurdate', date)\n",
    "        if date <= max(models_dates) and date <= datetime.date(2009, 1, 27):\n",
    "            cur_topics = defaultdict(int)\n",
    "            min_model_date = min([d for d in models_dates if d >= date])\n",
    "            if current_model_date < min_model_date:\n",
    "                model = load_model(model_dict[min_model_date])\n",
    "                current_model_date = min_model_date\n",
    "                print(current_model_date)\n",
    "#             print('making bow...')\n",
    "            par_bows = [dictionary.doc2bow(list(chain(*par))) for par in paragraphs]\n",
    "#             print('Done')\n",
    "            for bow in par_bows:\n",
    "#                 print('getting topics for bow')\n",
    "                topics = model.get_document_topics(bow, minimum_probability=0.5)\n",
    "#                 pp(topics)\n",
    "                for topic, prob in sorted(topics, key=lambda  x: -x[1])[:5]:\n",
    "                    topic_terms = '+'.join([w for w, p in model.show_topic(topic, 5)])\n",
    "                    cur_topics[topic_terms] += 1  # ID топиков со временем меняются, поэтому используем слова в к-ве id\n",
    "            if cur_topics:\n",
    "                doc_topics = sorted(cur_topics.items(), key=lambda x: -x[1])[:5]\n",
    "                print(doc_topics)\n",
    "#                 print(doc_topics)\n",
    "                for topic, prob in doc_topics:\n",
    "                    topic_by_date_counts[date][topic] += 1\n",
    "                    date_by_topic_counts[topic][date] += 1\n",
    "            \n",
    "    return topic_by_date_counts, date_by_topic_counts\n",
    "        \n",
    "tbd, dbt = get_topic_distributions(test_dict, test_corpus,  models_dict)\n",
    "dbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('-', 'b'),\n",
      " ('-', 'g'),\n",
      " ('-', 'r'),\n",
      " ('-', 'c'),\n",
      " ('-', 'm'),\n",
      " ('-', 'y'),\n",
      " ('-', 'k'),\n",
      " ('--', 'b'),\n",
      " ('--', 'g'),\n",
      " ('--', 'r'),\n",
      " ('--', 'c'),\n",
      " ('--', 'm'),\n",
      " ('--', 'y'),\n",
      " ('--', 'k'),\n",
      " (':', 'b'),\n",
      " (':', 'g'),\n",
      " (':', 'r'),\n",
      " (':', 'c'),\n",
      " (':', 'm'),\n",
      " (':', 'y'),\n",
      " (':', 'k')]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "pop index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-7fe8d2800bd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[0mdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtbd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mplot_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdbt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-74-7fe8d2800bd3>\u001b[0m in \u001b[0;36mplot_topics\u001b[1;34m(date_by_topic_counts, dates)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdate_by_topic_counts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mrand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline_styles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline_styles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;31m#         line_styles.pop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mtopic_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdate_by_topic_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdates\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: pop index out of range"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "%matplotlib qt \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import spline\n",
    "from matplotlib.lines import Line2D\n",
    "from itertools import product\n",
    "linestyles = ['-', '--', ':']\n",
    "markers = []\n",
    "import random\n",
    "for m in Line2D.markers:\n",
    "    try:\n",
    "        if len(m) == 1 and m != ' ':\n",
    "            markers.append(m)\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "styles = markers + [\n",
    "    r'$\\lambda$',\n",
    "    r'$\\bowtie$',\n",
    "    r'$\\circlearrowleft$',\n",
    "    r'$\\clubsuit$',\n",
    "    r'$\\checkmark$']\n",
    "colors = ('b', 'g', 'r', 'c', 'm', 'y', 'k')\n",
    "\n",
    "\n",
    "\n",
    "def plot_topics(date_by_topic_counts, dates):\n",
    "    line_styles = list(product(linestyles, colors))\n",
    "    pp(line_styles)\n",
    "    # line_styles\n",
    "    datesecs =  [date.toordinal() for date in dates]\n",
    "    width = 12\n",
    "    height = 8\n",
    "    plt.figure(figsize=(width, height))\n",
    "    plt.ylim(0, 8)\n",
    "    legend = []\n",
    "    for topic in date_by_topic_counts:\n",
    "        rand = random.randint(0, len(line_styles))\n",
    "        line, color = line_styles.pop(rand)\n",
    "#         line_styles.pop\n",
    "        topic_counts = [date_by_topic_counts[topic].get(date, 0) for date in dates]\n",
    "#         xnew = np.linspace(min(datesecs),max(datesecs), 5000)\n",
    "#         topic_counts_sm = spline(datesecs ,topic_counts, xnew)\n",
    "        plt.plot(dates, topic_counts, line, color=color, markersize=10)\n",
    "#         plt.plot(xnew, topic_counts_sm)\n",
    "        legend.append(topic[:50])\n",
    "#         break\n",
    "    plt.plot(dates, topic_counts)\n",
    "    plt.legend(legend)\n",
    "    plt.show()\n",
    "\n",
    "dates = sorted(tbd.keys())\n",
    "plot_topics(dbt, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start gathering\n",
      "model loaded\n",
      "[('imac+nvidia+optical+box+aluminum', 1), ('inside+banner+black+together+height', 1)]\n",
      "[('imac+nvidia+optical+box+aluminum', 2)]\n",
      "[('multi-touch+response+return+often+else', 1), ('built-in+cellphone+output+suite+dropped', 1)]\n",
      "[('multi-touch+response+return+often+else', 1), ('built-in+cellphone+output+suite+dropped', 1)]\n",
      "[('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('mark+november+net+december+holiday', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 1)]\n",
      "[('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('unique+limit+expense+try+basic', 1)]\n",
      "[('mark+november+net+december+holiday', 1)]\n",
      "[('picture+perhaps+500+can’t+ain’t', 1), ('imac+nvidia+optical+box+aluminum', 1)]\n",
      "[('intel+probably+individual+feed+...', 1)]\n",
      "[('intel+probably+individual+feed+...', 1)]\n",
      "[('beyond+outside+leave+capable+finger', 2), ('built-in+cellphone+output+suite+dropped', 1), ('mark+november+net+december+holiday', 1), ('iwork+seem+cloud+remain+native', 1), ('macworld+yesterday+period+regarding+figure', 1)]\n",
      "[('intel+probably+individual+feed+...', 1)]\n",
      "[('inside+banner+black+together+height', 1)]\n",
      "[('unique+limit+expense+try+basic', 1)]\n",
      "[('beyond+outside+leave+capable+finger', 2), ('built-in+cellphone+output+suite+dropped', 1), ('mark+november+net+december+holiday', 1), ('iwork+seem+cloud+remain+native', 1), ('macworld+yesterday+period+regarding+figure', 1)]\n",
      "[('intel+probably+individual+feed+...', 1)]\n",
      "[('inside+banner+black+together+height', 1)]\n",
      "[('unique+limit+expense+try+basic', 1)]\n",
      "[('imac+nvidia+optical+box+aluminum', 1)]\n",
      "[('heavier+class+conjunction+written+cover', 1), ('hd+white+upon+complaint+60', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1), ('heavier+class+conjunction+written+cover', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('helped+faster+meanwhile+significantly+graphic', 1)]\n",
      "[('helped+faster+meanwhile+significantly+graphic', 1)]\n",
      "[('hd+white+upon+complaint+60', 1)]\n",
      "[('hd+white+upon+complaint+60', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 1), ('method+signal+ring+describes+acknowledges', 1), ('inside+banner+black+together+height', 1), ('beyond+outside+leave+capable+finger', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 8)]\n",
      "[('mark+november+net+december+holiday', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1), ('built-in+cellphone+output+suite+dropped', 1), ('intel+probably+individual+feed+...', 1)]\n",
      "[('helped+faster+meanwhile+significantly+graphic', 1)]\n",
      "[('mark+november+net+december+holiday', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1), ('built-in+cellphone+output+suite+dropped', 1), ('heavier+class+conjunction+written+cover', 1), ('close+opening+cap+elastic+technique', 1), ('hd+white+upon+complaint+60', 1)]\n",
      "[('hd+white+upon+complaint+60', 1)]\n",
      "[('mark+november+net+december+holiday', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 2)]\n",
      "[('mark+november+net+december+holiday', 1)]\n",
      "[('mark+november+net+december+holiday', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1), ('built-in+cellphone+output+suite+dropped', 1), ('heavier+class+conjunction+written+cover', 1), ('close+opening+cap+elastic+technique', 1), ('hd+white+upon+complaint+60', 1)]\n",
      "[('hd+white+upon+complaint+60', 1)]\n",
      "[('mark+november+net+december+holiday', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 2)]\n",
      "[('mark+november+net+december+holiday', 1)]\n",
      "[('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('hd+white+upon+complaint+60', 1)]\n",
      "[('helped+faster+meanwhile+significantly+graphic', 4), ('heavier+class+conjunction+written+cover', 2), ('ilife+imovie+charge+easily+entirely', 1), ('hd+white+upon+complaint+60', 1)]\n",
      "[('2007+movie+rarely+accessory+imovie', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 1)]\n",
      "[('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1), ('iwork+seem+cloud+remain+native', 1)]\n",
      "[('2007+movie+rarely+accessory+imovie', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 1)]\n",
      "[('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1), ('iwork+seem+cloud+remain+native', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('imac+nvidia+optical+box+aluminum', 1), ('intel+probably+individual+feed+...', 1)]\n",
      "[('close+opening+cap+elastic+technique', 1)]\n",
      "[('hd+white+upon+complaint+60', 5), ('helped+faster+meanwhile+significantly+graphic', 1), ('method+signal+ring+describes+acknowledges', 1), ('macworld+yesterday+period+regarding+figure', 1), ('desktop+mini+unibody+displayport+rest', 1)]\n",
      "[('built-in+cellphone+output+suite+dropped', 1)]\n",
      "[('iwork+seem+cloud+remain+native', 1)]\n",
      "[('built-in+cellphone+output+suite+dropped', 1)]\n",
      "[('iwork+seem+cloud+remain+native', 1)]\n",
      "[('built-in+cellphone+output+suite+dropped', 1)]\n",
      "[('iwork+seem+cloud+remain+native', 1), ('ilife+imovie+charge+easily+entirely', 1), ('hd+white+upon+complaint+60', 1)]\n",
      "[('imac+nvidia+optical+box+aluminum', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('intel+probably+individual+feed+...', 2)]\n",
      "[('mark+november+net+december+holiday', 2), ('imac+nvidia+optical+box+aluminum', 1)]\n",
      "[('inside+banner+black+together+height', 1)]\n",
      "[('built-in+cellphone+output+suite+dropped', 1)]\n",
      "[('beyond+outside+leave+capable+finger', 1)]\n",
      "[('inside+banner+black+together+height', 1)]\n",
      "[('built-in+cellphone+output+suite+dropped', 1)]\n",
      "[('beyond+outside+leave+capable+finger', 1)]\n",
      "[('neutral+face+environment+commercial+document', 1), ('desktop+mini+unibody+displayport+rest', 1)]\n",
      "[('built-in+cellphone+output+suite+dropped', 2), ('inside+banner+black+together+height', 1), ('desktop+mini+unibody+displayport+rest', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 1)]\n",
      "[('desktop+mini+unibody+displayport+rest', 1)]\n",
      "[('2007+movie+rarely+accessory+imovie', 1), ('hd+white+upon+complaint+60', 1)]\n",
      "[('2007+movie+rarely+accessory+imovie', 1), ('hd+white+upon+complaint+60', 1)]\n",
      "[('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('intel+probably+individual+feed+...', 1)]\n",
      "[('helped+faster+meanwhile+significantly+graphic', 1)]\n",
      "[('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('helped+faster+meanwhile+significantly+graphic', 1)]\n",
      "[('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('built-in+cellphone+output+suite+dropped', 1)]\n",
      "[('mark+november+net+december+holiday', 1)]\n",
      "[('2007+movie+rarely+accessory+imovie', 2), ('unique+limit+expense+try+basic', 1), ('built-in+cellphone+output+suite+dropped', 1), ('desktop+mini+unibody+displayport+rest', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 2), ('mark+november+net+december+holiday', 1), ('hd+white+upon+complaint+60', 1)]\n",
      "[('2007+movie+rarely+accessory+imovie', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 1)]\n",
      "[('desktop+mini+unibody+displayport+rest', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('2007+movie+rarely+accessory+imovie', 1), ('iwork+seem+cloud+remain+native', 1), ('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('2007+movie+rarely+accessory+imovie', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 1)]\n",
      "[('desktop+mini+unibody+displayport+rest', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('2007+movie+rarely+accessory+imovie', 1), ('iwork+seem+cloud+remain+native', 1), ('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('mark+november+net+december+holiday', 1), ('unique+limit+expense+try+basic', 1)]\n",
      "[('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('intel+probably+individual+feed+...', 2)]\n",
      "[('imac+nvidia+optical+box+aluminum', 1), ('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('hd+white+upon+complaint+60', 2)]\n",
      "[('picture+perhaps+500+can’t+ain’t', 1), ('imac+nvidia+optical+box+aluminum', 1)]\n",
      "[('hd+white+upon+complaint+60', 2)]\n",
      "[('picture+perhaps+500+can’t+ain’t', 1), ('imac+nvidia+optical+box+aluminum', 1)]\n",
      "[('hear+outlined+fill+lead-up+0', 2), ('helped+faster+meanwhile+significantly+graphic', 1), ('2007+movie+rarely+accessory+imovie', 1), ('imac+nvidia+optical+box+aluminum', 1)]\n",
      "[('neutral+face+environment+commercial+document', 1), ('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 1), ('intel+probably+individual+feed+...', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 1)]\n",
      "[('unique+limit+expense+try+basic', 1), ('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('hear+outlined+fill+lead-up+0', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 1)]\n",
      "[('unique+limit+expense+try+basic', 1), ('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('hear+outlined+fill+lead-up+0', 1)]\n",
      "[('heavier+class+conjunction+written+cover', 1)]\n",
      "[('multi-touch+response+return+often+else', 1), ('patent+filed+pcs+office+potentially', 1), ('hd+white+upon+complaint+60', 1), ('imac+nvidia+optical+box+aluminum', 1), ('intel+probably+individual+feed+...', 1)]\n",
      "[('beyond+outside+leave+capable+finger', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1), ('unique+limit+expense+try+basic', 1)]\n",
      "[('2007+movie+rarely+accessory+imovie', 1)]\n",
      "[('beyond+outside+leave+capable+finger', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1), ('unique+limit+expense+try+basic', 1)]\n",
      "[('2007+movie+rarely+accessory+imovie', 1)]\n",
      "[('mark+november+net+december+holiday', 1), ('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('intel+probably+individual+feed+...', 1)]\n",
      "[('intel+probably+individual+feed+...', 1), ('macworld+yesterday+period+regarding+figure', 1)]\n",
      "[('built-in+cellphone+output+suite+dropped', 1), ('imac+nvidia+optical+box+aluminum', 1), ('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1), ('mark+november+net+december+holiday', 1), ('iwork+seem+cloud+remain+native', 1)]\n",
      "[('built-in+cellphone+output+suite+dropped', 1), ('close+opening+cap+elastic+technique', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1), ('imac+nvidia+optical+box+aluminum', 1), ('desktop+mini+unibody+displayport+rest', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 2)]\n",
      "[('method+signal+ring+describes+acknowledges', 1), ('imac+nvidia+optical+box+aluminum', 1), ('desktop+mini+unibody+displayport+rest', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 2)]\n",
      "[('intel+probably+individual+feed+...', 1)]\n",
      "[('unique+limit+expense+try+basic', 1)]\n",
      "[('beyond+outside+leave+capable+finger', 1), ('hd+white+upon+complaint+60', 1), ('desktop+mini+unibody+displayport+rest', 1)]\n",
      "[('hd+white+upon+complaint+60', 1)]\n",
      "[('intel+probably+individual+feed+...', 3), ('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('desktop+mini+unibody+displayport+rest', 2), ('intel+probably+individual+feed+...', 2), ('multi-touch+response+return+often+else', 1), ('built-in+cellphone+output+suite+dropped', 1), ('imac+nvidia+optical+box+aluminum', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('hd+white+upon+complaint+60', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('hd+white+upon+complaint+60', 1)]\n",
      "[('neutral+face+environment+commercial+document', 1)]\n",
      "[('mark+november+net+december+holiday', 1), ('imac+nvidia+optical+box+aluminum', 1), ('beyond+outside+leave+capable+finger', 1)]\n",
      "[('beyond+outside+leave+capable+finger', 1)]\n",
      "[('close+opening+cap+elastic+technique', 1), ('imac+nvidia+optical+box+aluminum', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 1), ('helped+faster+meanwhile+significantly+graphic', 1), ('internet+isn’t+mobileme+moving+learn', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1), ('beyond+outside+leave+capable+finger', 1)]\n",
      "[('close+opening+cap+elastic+technique', 1)]\n",
      "[('hd+white+upon+complaint+60', 1)]\n",
      "[('mark+november+net+december+holiday', 4), ('patent+filed+pcs+office+potentially', 1), ('ilife+imovie+charge+easily+entirely', 1), ('inside+banner+black+together+height', 1), ('built-in+cellphone+output+suite+dropped', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1), ('beyond+outside+leave+capable+finger', 1)]\n",
      "[('close+opening+cap+elastic+technique', 1)]\n",
      "[('hd+white+upon+complaint+60', 1)]\n",
      "[('mark+november+net+december+holiday', 4), ('patent+filed+pcs+office+potentially', 1), ('ilife+imovie+charge+easily+entirely', 1), ('inside+banner+black+together+height', 1), ('built-in+cellphone+output+suite+dropped', 1)]\n",
      "[('close+opening+cap+elastic+technique', 1)]\n",
      "[('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 1)]\n",
      "[('picture+perhaps+500+can’t+ain’t', 1), ('imac+nvidia+optical+box+aluminum', 1)]\n",
      "[('imac+nvidia+optical+box+aluminum', 1)]\n",
      "[('imac+nvidia+optical+box+aluminum', 1)]\n",
      "[('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('heavier+class+conjunction+written+cover', 1), ('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('hd+white+upon+complaint+60', 5), ('patent+filed+pcs+office+potentially', 1), ('helped+faster+meanwhile+significantly+graphic', 1), ('close+opening+cap+elastic+technique', 1), ('neutral+face+environment+commercial+document', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('neutral+face+environment+commercial+document', 1), ('hd+white+upon+complaint+60', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('neutral+face+environment+commercial+document', 1), ('hd+white+upon+complaint+60', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('mark+november+net+december+holiday', 1), ('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 1)]\n",
      "[('patent+filed+pcs+office+potentially', 1), ('method+signal+ring+describes+acknowledges', 1), ('heavier+class+conjunction+written+cover', 1)]\n",
      "[('2007+movie+rarely+accessory+imovie', 1)]\n",
      "[('imac+nvidia+optical+box+aluminum', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1), ('close+opening+cap+elastic+technique', 1)]\n",
      "[('neutral+face+environment+commercial+document', 1), ('patent+filed+pcs+office+potentially', 1), ('intel+probably+individual+feed+...', 1)]\n",
      "[('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('built-in+cellphone+output+suite+dropped', 1)]\n",
      "[('ilife+imovie+charge+easily+entirely', 1)]\n",
      "[('built-in+cellphone+output+suite+dropped', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1), ('intel+probably+individual+feed+...', 1)]\n",
      "[('intel+probably+individual+feed+...', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('intel+probably+individual+feed+...', 1)]\n",
      "[('hd+white+upon+complaint+60', 1)]\n",
      "[('method+signal+ring+describes+acknowledges', 1)]\n",
      "[('intel+probably+individual+feed+...', 1)]\n",
      "[('hd+white+upon+complaint+60', 1)]\n",
      "[('unique+limit+expense+try+basic', 1), ('mark+november+net+december+holiday', 1)]\n"
     ]
    }
   ],
   "source": [
    "cc_models_dict = load_topic_models('paragraphs_unconverged_model3')\n",
    "cc_tbd, cc_dbt = get_topic_distributions(cc_dict, cc_docs,  cc_models_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.get_topic_distributions.<locals>.<lambda>>,\n",
       "            {'2007+movie+rarely+accessory+imovie': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 7): 2,\n",
       "                          datetime.date(2009, 1, 11): 2,\n",
       "                          datetime.date(2009, 1, 13): 5,\n",
       "                          datetime.date(2009, 1, 15): 1,\n",
       "                          datetime.date(2009, 1, 16): 2,\n",
       "                          datetime.date(2009, 1, 25): 1}),\n",
       "             'beyond+outside+leave+capable+finger': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 4): 2,\n",
       "                          datetime.date(2009, 1, 6): 1,\n",
       "                          datetime.date(2009, 1, 9): 2,\n",
       "                          datetime.date(2009, 1, 16): 2,\n",
       "                          datetime.date(2009, 1, 20): 1,\n",
       "                          datetime.date(2009, 1, 21): 4}),\n",
       "             'built-in+cellphone+output+suite+dropped': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 1): 2,\n",
       "                          datetime.date(2009, 1, 4): 2,\n",
       "                          datetime.date(2009, 1, 6): 3,\n",
       "                          datetime.date(2009, 1, 8): 3,\n",
       "                          datetime.date(2009, 1, 9): 2,\n",
       "                          datetime.date(2009, 1, 10): 1,\n",
       "                          datetime.date(2009, 1, 12): 1,\n",
       "                          datetime.date(2009, 1, 13): 1,\n",
       "                          datetime.date(2009, 1, 18): 1,\n",
       "                          datetime.date(2009, 1, 19): 1,\n",
       "                          datetime.date(2009, 1, 20): 1,\n",
       "                          datetime.date(2009, 1, 21): 2,\n",
       "                          datetime.date(2009, 1, 26): 2}),\n",
       "             'close+opening+cap+elastic+technique': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 6): 2,\n",
       "                          datetime.date(2009, 1, 8): 1,\n",
       "                          datetime.date(2009, 1, 19): 1,\n",
       "                          datetime.date(2009, 1, 21): 4,\n",
       "                          datetime.date(2009, 1, 23): 1,\n",
       "                          datetime.date(2009, 1, 26): 1}),\n",
       "             'desktop+mini+unibody+displayport+rest': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 8): 1,\n",
       "                          datetime.date(2009, 1, 10): 2,\n",
       "                          datetime.date(2009, 1, 11): 1,\n",
       "                          datetime.date(2009, 1, 13): 3,\n",
       "                          datetime.date(2009, 1, 19): 2,\n",
       "                          datetime.date(2009, 1, 20): 2}),\n",
       "             'hd+white+upon+complaint+60': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 5): 3,\n",
       "                          datetime.date(2009, 1, 6): 5,\n",
       "                          datetime.date(2009, 1, 7): 1,\n",
       "                          datetime.date(2009, 1, 8): 2,\n",
       "                          datetime.date(2009, 1, 11): 2,\n",
       "                          datetime.date(2009, 1, 13): 1,\n",
       "                          datetime.date(2009, 1, 14): 2,\n",
       "                          datetime.date(2009, 1, 16): 1,\n",
       "                          datetime.date(2009, 1, 20): 4,\n",
       "                          datetime.date(2009, 1, 21): 2,\n",
       "                          datetime.date(2009, 1, 23): 3,\n",
       "                          datetime.date(2009, 1, 27): 2}),\n",
       "             'hear+outlined+fill+lead-up+0': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 15): 3}),\n",
       "             'heavier+class+conjunction+written+cover': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 5): 2,\n",
       "                          datetime.date(2009, 1, 6): 2,\n",
       "                          datetime.date(2009, 1, 7): 1,\n",
       "                          datetime.date(2009, 1, 15): 1,\n",
       "                          datetime.date(2009, 1, 22): 1,\n",
       "                          datetime.date(2009, 1, 24): 1}),\n",
       "             'helped+faster+meanwhile+significantly+graphic': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 5): 2,\n",
       "                          datetime.date(2009, 1, 6): 1,\n",
       "                          datetime.date(2009, 1, 7): 1,\n",
       "                          datetime.date(2009, 1, 8): 1,\n",
       "                          datetime.date(2009, 1, 12): 2,\n",
       "                          datetime.date(2009, 1, 15): 1,\n",
       "                          datetime.date(2009, 1, 21): 1,\n",
       "                          datetime.date(2009, 1, 23): 1}),\n",
       "             'ilife+imovie+charge+easily+entirely': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 1): 1,\n",
       "                          datetime.date(2009, 1, 3): 1,\n",
       "                          datetime.date(2009, 1, 6): 1,\n",
       "                          datetime.date(2009, 1, 7): 3,\n",
       "                          datetime.date(2009, 1, 8): 1,\n",
       "                          datetime.date(2009, 1, 11): 1,\n",
       "                          datetime.date(2009, 1, 12): 2,\n",
       "                          datetime.date(2009, 1, 13): 3,\n",
       "                          datetime.date(2009, 1, 14): 1,\n",
       "                          datetime.date(2009, 1, 15): 2,\n",
       "                          datetime.date(2009, 1, 16): 1,\n",
       "                          datetime.date(2009, 1, 18): 1,\n",
       "                          datetime.date(2009, 1, 21): 3,\n",
       "                          datetime.date(2009, 1, 22): 2,\n",
       "                          datetime.date(2009, 1, 23): 2,\n",
       "                          datetime.date(2009, 1, 26): 2}),\n",
       "             'imac+nvidia+optical+box+aluminum': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 1): 2,\n",
       "                          datetime.date(2009, 1, 4): 1,\n",
       "                          datetime.date(2009, 1, 5): 1,\n",
       "                          datetime.date(2009, 1, 7): 1,\n",
       "                          datetime.date(2009, 1, 8): 1,\n",
       "                          datetime.date(2009, 1, 9): 1,\n",
       "                          datetime.date(2009, 1, 14): 3,\n",
       "                          datetime.date(2009, 1, 15): 1,\n",
       "                          datetime.date(2009, 1, 16): 1,\n",
       "                          datetime.date(2009, 1, 18): 1,\n",
       "                          datetime.date(2009, 1, 19): 2,\n",
       "                          datetime.date(2009, 1, 20): 1,\n",
       "                          datetime.date(2009, 1, 21): 2,\n",
       "                          datetime.date(2009, 1, 22): 3,\n",
       "                          datetime.date(2009, 1, 25): 1}),\n",
       "             'inside+banner+black+together+height': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 1): 1,\n",
       "                          datetime.date(2009, 1, 4): 2,\n",
       "                          datetime.date(2009, 1, 6): 1,\n",
       "                          datetime.date(2009, 1, 9): 2,\n",
       "                          datetime.date(2009, 1, 10): 1,\n",
       "                          datetime.date(2009, 1, 21): 2}),\n",
       "             'intel+probably+individual+feed+...': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 4): 4,\n",
       "                          datetime.date(2009, 1, 6): 1,\n",
       "                          datetime.date(2009, 1, 7): 1,\n",
       "                          datetime.date(2009, 1, 9): 1,\n",
       "                          datetime.date(2009, 1, 12): 1,\n",
       "                          datetime.date(2009, 1, 13): 1,\n",
       "                          datetime.date(2009, 1, 15): 1,\n",
       "                          datetime.date(2009, 1, 16): 2,\n",
       "                          datetime.date(2009, 1, 17): 1,\n",
       "                          datetime.date(2009, 1, 19): 1,\n",
       "                          datetime.date(2009, 1, 20): 2,\n",
       "                          datetime.date(2009, 1, 26): 1,\n",
       "                          datetime.date(2009, 1, 27): 4}),\n",
       "             'internet+isn’t+mobileme+moving+learn': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 21): 1}),\n",
       "             'iwork+seem+cloud+remain+native': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 4): 2,\n",
       "                          datetime.date(2009, 1, 7): 2,\n",
       "                          datetime.date(2009, 1, 8): 3,\n",
       "                          datetime.date(2009, 1, 13): 2,\n",
       "                          datetime.date(2009, 1, 19): 1}),\n",
       "             'macworld+yesterday+period+regarding+figure': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 4): 2,\n",
       "                          datetime.date(2009, 1, 8): 1,\n",
       "                          datetime.date(2009, 1, 17): 1}),\n",
       "             'mark+november+net+december+holiday': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 2): 1,\n",
       "                          datetime.date(2009, 1, 3): 1,\n",
       "                          datetime.date(2009, 1, 4): 2,\n",
       "                          datetime.date(2009, 1, 6): 7,\n",
       "                          datetime.date(2009, 1, 9): 1,\n",
       "                          datetime.date(2009, 1, 12): 1,\n",
       "                          datetime.date(2009, 1, 13): 2,\n",
       "                          datetime.date(2009, 1, 16): 1,\n",
       "                          datetime.date(2009, 1, 19): 1,\n",
       "                          datetime.date(2009, 1, 21): 3,\n",
       "                          datetime.date(2009, 1, 23): 1,\n",
       "                          datetime.date(2009, 1, 27): 1}),\n",
       "             'method+signal+ring+describes+acknowledges': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 5): 2,\n",
       "                          datetime.date(2009, 1, 6): 7,\n",
       "                          datetime.date(2009, 1, 7): 3,\n",
       "                          datetime.date(2009, 1, 8): 2,\n",
       "                          datetime.date(2009, 1, 13): 2,\n",
       "                          datetime.date(2009, 1, 15): 1,\n",
       "                          datetime.date(2009, 1, 16): 2,\n",
       "                          datetime.date(2009, 1, 19): 5,\n",
       "                          datetime.date(2009, 1, 20): 3,\n",
       "                          datetime.date(2009, 1, 21): 2,\n",
       "                          datetime.date(2009, 1, 23): 6,\n",
       "                          datetime.date(2009, 1, 24): 1,\n",
       "                          datetime.date(2009, 1, 26): 2,\n",
       "                          datetime.date(2009, 1, 27): 3}),\n",
       "             'multi-touch+response+return+often+else': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 1): 2,\n",
       "                          datetime.date(2009, 1, 16): 1,\n",
       "                          datetime.date(2009, 1, 20): 1}),\n",
       "             'neutral+face+environment+commercial+document': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 10): 1,\n",
       "                          datetime.date(2009, 1, 15): 1,\n",
       "                          datetime.date(2009, 1, 20): 1,\n",
       "                          datetime.date(2009, 1, 23): 3,\n",
       "                          datetime.date(2009, 1, 26): 1}),\n",
       "             'patent+filed+pcs+office+potentially': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 2): 2,\n",
       "                          datetime.date(2009, 1, 6): 1,\n",
       "                          datetime.date(2009, 1, 7): 2,\n",
       "                          datetime.date(2009, 1, 10): 1,\n",
       "                          datetime.date(2009, 1, 13): 3,\n",
       "                          datetime.date(2009, 1, 15): 3,\n",
       "                          datetime.date(2009, 1, 16): 1,\n",
       "                          datetime.date(2009, 1, 21): 3,\n",
       "                          datetime.date(2009, 1, 22): 1,\n",
       "                          datetime.date(2009, 1, 23): 1,\n",
       "                          datetime.date(2009, 1, 24): 2,\n",
       "                          datetime.date(2009, 1, 26): 1}),\n",
       "             'picture+perhaps+500+can’t+ain’t': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 4): 1,\n",
       "                          datetime.date(2009, 1, 14): 2,\n",
       "                          datetime.date(2009, 1, 22): 1}),\n",
       "             'unique+limit+expense+try+basic': defaultdict(int,\n",
       "                         {datetime.date(2009, 1, 3): 1,\n",
       "                          datetime.date(2009, 1, 4): 2,\n",
       "                          datetime.date(2009, 1, 13): 2,\n",
       "                          datetime.date(2009, 1, 15): 2,\n",
       "                          datetime.date(2009, 1, 16): 2,\n",
       "                          datetime.date(2009, 1, 19): 1,\n",
       "                          datetime.date(2009, 1, 27): 1})})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_dbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-aff34f5b32ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib qt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcc_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcc_tbd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplot_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcc_dbt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcc_dates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-57-efe2170e0970>\u001b[0m in \u001b[0;36mplot_topics\u001b[1;34m(date_by_topic_counts, dates)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdate_by_topic_counts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mrand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline_styles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline_styles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;31m#         line_styles.pop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mtopic_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdate_by_topic_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdates\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: pop index out of range"
     ]
    }
   ],
   "source": [
    "%matplotlib qt \n",
    "cc_dates = sorted(cc_tbd.keys())\n",
    "plot_topics(cc_dbt, cc_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Считаем статистику по топикам в коллекции**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_topics(paragrsph_split_texts, dictionary, threshold):\n",
    "    \"\"\"\n",
    "    Counts the number of times each topic from a given model occurred in each of the bow_texts with corresponding \n",
    "    dates from the dates array.\n",
    "    \"\"\"\n",
    "    date_counts = defaultdict(lambda: defaultdict(int))\n",
    "    overall_counts = defaultdict(int)\n",
    "    for id, date, paragraphs in paragrsph_split_texts:\n",
    "        for paragraph in paragraphs:\n",
    "            if type(paragraph) == list:\n",
    "                bow = dictionary.doc2bow(list(chain(*paragraph)))\n",
    "            else:    \n",
    "                bow = dictionary.doc2bow(paragraph)\n",
    "            if not len(paragraph) == 0:\n",
    "                topics = model.get_document_topics(bow, minimum_probability=threshold)\n",
    "                for topic, prob in topics:\n",
    "                    date_counts[date][topic] += 1\n",
    "                    overall_counts[topic] += 1\n",
    "    return date_counts, sorted(overall_counts.items(), key=lambda x: -x[1])\n",
    "\n",
    "def topics_words(model, sprted_topic_counts, topics_number=20, words_number=10):\n",
    "    terms = []\n",
    "    for topic, count in sprted_topic_counts[:topics_number]:\n",
    "        most_frequent_words = model.show_topic(topic, words_number)\n",
    "        terms.append([topic, count, ', '.join([word for word, prob in most_frequent_words])])\n",
    "    return terms\n",
    "\n",
    "test_date_counts, test_topic_counts = count_topics(test_corpus, test_dict, 0.5)\n",
    "topics_words(model, test_topic_counts)-"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
